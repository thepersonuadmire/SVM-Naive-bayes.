{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0i9j0E2i6RMN"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN3q+McvKYY0DZi6ElCfl8y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thepersonuadmire/SVM-Naive-bayes./blob/main/SVM_%26_Naive_bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Theoretical"
      ],
      "metadata": {
        "id": "0i9j0E2i6RMN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a Support Vector Machine (SVM)?\n"
      ],
      "metadata": {
        "id": "OdSa4uik5X30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM is a supervised machine learning algorithm used for classification and regression tasks. It finds the optimal hyperplane that separates data points of different classes with the maximum margin."
      ],
      "metadata": {
        "id": "414mVdKJFG32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between Hard Margin and Soft Margin SVM?\n"
      ],
      "metadata": {
        "id": "U7Reiu3F5laW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hard Margin SVM: Assumes data is perfectly separable and does not allow any misclassifications.\n",
        "\n",
        "Soft Margin SVM: Allows some misclassifications to handle noisy or non-separable data using a regularization parameter ( C )."
      ],
      "metadata": {
        "id": "HbJzZr_eFIxL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the mathematical intuition behind SVM?\n"
      ],
      "metadata": {
        "id": "9CVH0Iyb5nPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM aims to maximize the margin between the hyperplane and the nearest data points (support vectors) by solving a constrained optimization problem."
      ],
      "metadata": {
        "id": "gkF_uOjZFMWx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the role of Lagrange Multipliers in SVM?\n"
      ],
      "metadata": {
        "id": "dcUjLWm75pUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lagrange multipliers are used to solve the constrained optimization problem in SVM by converting it into a dual problem, which is easier to solve."
      ],
      "metadata": {
        "id": "2_9BTbphFRLp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are Support Vectors in SVM?\n"
      ],
      "metadata": {
        "id": "tJ_rX7le5rUB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support vectors are the data points closest to the hyperplane that influence its position and orientation. They are critical for defining the margin."
      ],
      "metadata": {
        "id": "y15kULvLFUhV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is a Support Vector Classifier (SVC)?\n"
      ],
      "metadata": {
        "id": "N9bkIngg5tqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVC is an SVM model used for classification tasks. It finds the hyperplane that best separates the classes."
      ],
      "metadata": {
        "id": "xbS_Fp3pFXc_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is a Support Vector Regressor (SVR)?\n"
      ],
      "metadata": {
        "id": "ZEMosfGy5vlV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVR is an SVM model used for regression tasks. It finds a hyperplane that fits the data while minimizing the error within a specified margin."
      ],
      "metadata": {
        "id": "D_mv76XbFZ9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the Kernel Trick in SVM?\n"
      ],
      "metadata": {
        "id": "7EuglJ2Q5xrT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The kernel trick allows SVM to operate in a higher-dimensional space without explicitly computing the transformation. It uses kernel functions to compute dot products in the transformed space."
      ],
      "metadata": {
        "id": "IqhjR6DeFced"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Compare Linear Kernel, Polynomial Kernel, and RBF Kernel.\n"
      ],
      "metadata": {
        "id": "Lv1dxyXD5zcN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Kernel: Works well for linearly separable data.\n",
        "\n",
        "Polynomial Kernel: Captures polynomial relationships between features.\n",
        "\n",
        "RBF Kernel: Handles non-linear data by mapping it to an infinite-dimensional space."
      ],
      "metadata": {
        "id": "Lfgw97ROFglr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is the effect of the C parameter in SVM?\n"
      ],
      "metadata": {
        "id": "bIMKfED351lj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "( C ) controls the trade-off between maximizing the margin and minimizing classification errors. A smaller ( C ) allows more misclassifications, while a larger ( C ) penalizes misclassifications heavily."
      ],
      "metadata": {
        "id": "9QLMSRKSFkrx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is the role of the Gamma parameter in RBF Kernel SVM?\n"
      ],
      "metadata": {
        "id": "E_PUqapY53bv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gamma defines the influence of a single training example. A small gamma means far points are considered, while a large gamma focuses on nearby points."
      ],
      "metadata": {
        "id": "RGTqnE-CFqag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n"
      ],
      "metadata": {
        "id": "kq1WGYQo55PT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naïve Bayes is a probabilistic classifier based on Bayes’ Theorem. It is called \"naïve\" because it assumes features are independent of each other."
      ],
      "metadata": {
        "id": "L2YK9-fGFuth"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is Bayes’ Theorem?\n"
      ],
      "metadata": {
        "id": "dxiNeF3B56-f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bayes’ Theorem calculates the probability of an event based on prior knowledge:\n",
        "( P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} ).\n",
        "\n"
      ],
      "metadata": {
        "id": "uioAlwjhFwVV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes.\n"
      ],
      "metadata": {
        "id": "hSZANHXz58s-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gaussian: Assumes features follow a normal distribution.\n",
        "\n",
        "Multinomial: Used for discrete data like word counts.\n",
        "\n",
        "Bernoulli: Used for binary features."
      ],
      "metadata": {
        "id": "Bgzg7WmWFzQz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. When should you use Gaussian Naïve Bayes over other variants?\n"
      ],
      "metadata": {
        "id": "-EG7SZDb5-sD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Gaussian Naïve Bayes when features are continuous and follow a normal distribution."
      ],
      "metadata": {
        "id": "aVhBcgyFF2ub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What are the key assumptions made by Naïve Bayes?\n"
      ],
      "metadata": {
        "id": "op4JmJ3c6ARk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Features are independent of each other.\n",
        "\n",
        "The presence of one feature does not affect the probability of another."
      ],
      "metadata": {
        "id": "Q9HvX5FRF5XV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are the advantages and disadvantages of Naïve Bayes?\n"
      ],
      "metadata": {
        "id": "suNT_G-i6CBR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advantages: Fast, simple, works well with high-dimensional data.\n",
        "\n",
        "Disadvantages: Assumes feature independence, which may not hold in real-world data."
      ],
      "metadata": {
        "id": "hCUJ2uOhF9Y2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Why is Naïve Bayes a good choice for text classification?\n"
      ],
      "metadata": {
        "id": "ftWIXRUf6Duk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It handles high-dimensional data well and performs efficiently with sparse datasets like text."
      ],
      "metadata": {
        "id": "9ZJwjsqdGAR2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Compare SVM and Naïve Bayes for classification tasks.\n"
      ],
      "metadata": {
        "id": "BhIYTzDI6FSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM: Better for complex, non-linear data but computationally expensive.\n",
        "\n",
        "Naïve Bayes: Faster and simpler but assumes feature independence."
      ],
      "metadata": {
        "id": "N9Lekz-4GC0X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How does Laplace Smoothing help in Naïve Bayes?"
      ],
      "metadata": {
        "id": "O4e9tVKa6HVf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Laplace Smoothing adds a small constant to avoid zero probabilities, ensuring that unseen features do not result in zero likelihood."
      ],
      "metadata": {
        "id": "UUw4X2xKGF0l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical"
      ],
      "metadata": {
        "id": "I6ei1pMA6IbU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy.\n"
      ],
      "metadata": {
        "id": "6BrmVRti5e4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
        "model = SVC(kernel='linear')\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "QEqVH9eiGKEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n"
      ],
      "metadata": {
        "id": "rwf1UQGB_c9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "wine = datasets.load_wine()\n",
        "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.3, random_state=42)\n",
        "\n",
        "model_linear = SVC(kernel='linear')\n",
        "model_linear.fit(X_train, y_train)\n",
        "y_pred_linear = model_linear.predict(X_test)\n",
        "print(\"Linear Kernel Accuracy:\", accuracy_score(y_test, y_pred_linear))\n",
        "\n",
        "model_rbf = SVC(kernel='rbf')\n",
        "model_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = model_rbf.predict(X_test)\n",
        "print(\"RBF Kernel Accuracy:\", accuracy_score(y_test, y_pred_rbf))"
      ],
      "metadata": {
        "id": "Xm_tJVPHGMdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean Squared Error (MSE)."
      ],
      "metadata": {
        "id": "60Qs3eub_fv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "boston = datasets.load_boston()\n",
        "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.3, random_state=42)\n",
        "model = SVR(kernel='linear')\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict (X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)"
      ],
      "metadata": {
        "id": "0Ng1vsrRGPMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "24. Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision boundary.\n"
      ],
      "metadata": {
        "id": "kQu1xnte_ikU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2]  # Use only the first two features for visualization\n",
        "y = iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = SVC(kernel='poly', degree=3)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Create a mesh to plot the decision boundary\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
        "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k', marker='o')\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c='red', edgecolors='k', marker='x')\n",
        "plt.title(\"SVM with Polynomial Kernel Decision Boundary\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "20G049k5GSJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n"
      ],
      "metadata": {
        "id": "bcY90m96_lM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "breast_cancer = datasets.load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(breast_cancer.data, breast_cancer.target, test_size=0.3, random_state=42)\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "SvVd9m7nGV6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Write a Python program to train a Multinomial Naïve Bayes classifier for text classification using the 20 Newsgroups dataset.\n"
      ],
      "metadata": {
        "id": "e8haryOv_n_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "newsgroups = fetch_20newsgroups(subset='all')\n",
        "X_train, X_test, y_train, y_test = train_test_split(newsgroups.data, newsgroups.target, test_size=0.3, random_state=42)\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_counts = vectorizer.fit_transform(X_train)\n",
        "X_test_counts = vectorizer.transform(X_test)\n",
        "\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_counts, y_train)\n",
        "y_pred = model.predict(X_test_counts)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "-a47lJ8iGZPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Write a Python program to train an SVM Classifier with different C values and compare the decision boundaries visually.\n"
      ],
      "metadata": {
        "id": "ugEsnIgd_qPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2]  # Use only the first two features for visualization\n",
        "y = iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "C_values = [0.1, 1, 10]\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "for i, C in enumerate(C_values):\n",
        "    model = SVC(kernel='linear', C=C)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    plt.subplot(1, 3, i + 1)\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k', marker='o')\n",
        "    plt.scatter(X_test[:, 0], X_test[:, 1], c='red', edgecolors='k', marker='x')\n",
        "    plt.title(f\"SVM with C={C}\")\n",
        "    plt.xlabel(\"Feature 1\")\n",
        "    plt.ylabel(\"Feature 2\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tdxdZTEOGcll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Write a Python program to train a Bernoulli Naïve Bayes classifier for binary classification on a dataset with binary features.\n"
      ],
      "metadata": {
        "id": "mipdgEHb_sfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10, n_clusters_per_class=1, random_state=42)\n",
        "X = (X > 0).astype(int)  # Convert to binary features\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = BernoulliNB()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "3bHOYZypGfYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Write a Python program to apply feature scaling before training an SVM model and compare results with unscaled data.\n"
      ],
      "metadata": {
        "id": "Gi09gD7P_uys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Without scaling\n",
        "model = SVC(kernel='linear')\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy without scaling:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# With scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model.predict(X_test_scaled)\n",
        "print(\"Accuracy with scaling:\", accuracy_score(y_test, y_pred_scaled))"
      ],
      "metadata": {
        "id": "k3gHcv5HGinD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Write a Python program to train a Gaussian Naïve Bayes model and compare the predictions before and after Laplace Smoothing.\n"
      ],
      "metadata": {
        "id": "v_8qilSw_xct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "breast_cancer = datasets.load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(breast_cancer.data, breast_cancer.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Without Laplace Smoothing\n",
        "model = GaussianNB(var_smoothing=0)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred_no_smoothing = model.predict(X_test)\n",
        "print(\"Accuracy without Laplace Smoothing:\", accuracy_score(y_test, y_pred_no_smoothing))\n",
        "\n",
        "# With Laplace Smoothing\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred_with_smoothing = model.predict(X_test)\n",
        "print(\"Accuracy with Laplace Smoothing:\", accuracy_score(y_test, y_pred_with_smoothing))"
      ],
      "metadata": {
        "id": "ValzoJntGlIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C, gamma, kernel).\n"
      ],
      "metadata": {
        "id": "jAAypcTt_zpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
        "\n",
        "param grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'gamma': ['scale', 'auto'],\n",
        "    'kernel': ['linear', 'rbf']\n",
        "}\n",
        "\n",
        "model = SVC()\n",
        "grid_search = GridSearchCV(model, param_grid=grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "mVR19OnfGnsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "32. Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting and check if it improves accuracy.\n"
      ],
      "metadata": {
        "id": "y1Z_HOwK_1-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, weights=[0.9, 0.1], random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Without class weighting\n",
        "model = SVC(kernel='linear')\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy without class weighting:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# With class weighting\n",
        "model_weighted = SVC(kernel='linear', class_weight='balanced')\n",
        "model_weighted.fit(X_train, y_train)\n",
        "y_pred_weighted = model_weighted.predict(X_test)\n",
        "print(\"Accuracy with class weighting:\", accuracy_score(y_test, y_pred_weighted))"
      ],
      "metadata": {
        "id": "96zyBXOJGqJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "33. Write a Python program to implement a Naïve Bayes classifier for spam detection using email data.\n"
      ],
      "metadata": {
        "id": "5bWt3K2d_5FY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Sample data\n",
        "data = {'text': ['Free money now', 'Hi, how are you?', 'Win a lottery', 'Meeting at 10am'],\n",
        "        'label': [1, 0, 1, 0]}  # 1: spam, 0: not spam\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.3, random_state=42)\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_counts = vectorizer.fit_transform(X_train)\n",
        "X_test_counts = vectorizer.transform(X_test)\n",
        "\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_counts, y_train)\n",
        "y_pred = model.predict(X_test_counts)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "cy5YocjEGuWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "34. Write a Python program to train an SVM Classifier and a Naïve Bayes Classifier on the same dataset and compare their accuracy.\n"
      ],
      "metadata": {
        "id": "_0AQjMGt_7Ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# SVM Classifier\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
        "\n",
        "# Naïve Bayes Classifier\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "y_pred_nb = nb_model.predict(X_test)\n",
        "print(\"Naïve Bayes Accuracy:\", accuracy_score(y_test, y_pred_nb))"
      ],
      "metadata": {
        "id": "ue-dhg4MGxZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "35. Write a Python program to perform feature selection before training a Naïve Bayes classifier and compare results.\n"
      ],
      "metadata": {
        "id": "xc9xTkh5_-aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Feature selection\n",
        "selector = SelectKBest(score_func=chi2, k =2)\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "\n",
        "# Train Naïve Bayes on selected features\n",
        "model = GaussianNB()\n",
        "model.fit(X_train_selected, y_train)\n",
        "y_pred_selected = model.predict(X_test_selected)\n",
        "print(\"Accuracy with feature selection:\", accuracy_score(y_test, y_pred_selected))\n",
        "\n",
        "# Train Naïve Bayes on all features\n",
        "model_all = GaussianNB()\n",
        "model_all.fit(X_train, y_train)\n",
        "y_pred_all = model_all.predict(X_test)\n",
        "print(\"Accuracy without feature selection:\", accuracy_score(y_test, y_pred_all))"
      ],
      "metadata": {
        "id": "MXx44ucHG1D_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "36. Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO) strategies on the Wine dataset and compare their accuracy.\n"
      ],
      "metadata": {
        "id": "NnDjElQaAArA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "wine = datasets.load_wine()\n",
        "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# One-vs-Rest\n",
        "ovr_model = OneVsRestClassifier(SVC(kernel='linear'))\n",
        "ovr_model.fit(X_train, y_train)\n",
        "y_pred_ovr = ovr_model.predict(X_test)\n",
        "print(\"One-vs-Rest Accuracy:\", accuracy_score(y_test, y_pred_ovr))\n",
        "\n",
        "# One-vs-One\n",
        "ovo_model = OneVsOneClassifier(SVC(kernel='linear'))\n",
        "ovo_model.fit(X_train, y_train)\n",
        "y_pred_ovo = ovo_model.predict(X_test)\n",
        "print(\"One-vs-One Accuracy:\", accuracy_score(y_test, y_pred_ovo))"
      ],
      "metadata": {
        "id": "zAbh0pPsG4Vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "37. Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast Cancer dataset and compare their accuracy.\n"
      ],
      "metadata": {
        "id": "AHlgM97rADAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "breast_cancer = datasets.load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(breast_cancer.data, breast_cancer.target, test_size=0.3, random_state=42)\n",
        "\n",
        "kernels = ['linear', 'poly', 'rbf']\n",
        "for kernel in kernels:\n",
        "    model = SVC(kernel=kernel)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"Accuracy with {kernel} kernel:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "yEQj4GSZG7E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "38. Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the average accuracy.\n"
      ],
      "metadata": {
        "id": "suiUxtQPAFVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "model = SVC(kernel='linear')\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "scores = cross_val_score(model, X, y, cv=skf)\n",
        "print(\"Average accuracy:\", scores.mean())"
      ],
      "metadata": {
        "id": "_thKO8vnG9O0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "39. Write a Python program to train a Naïve Bayes classifier using different prior probabilities and compare performance.\n"
      ],
      "metadata": {
        "id": "4BQjtfOjAHWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "breast_cancer = datasets.load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(breast_cancer.data, breast_cancer.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Default prior probabilities\n",
        "model_default = GaussianNB()\n",
        "model_default.fit(X_train, y_train)\n",
        "y_pred_default = model_default.predict(X_test)\n",
        "print(\"Accuracy with default priors:\", accuracy_score(y_test, y_pred_default))\n",
        "\n",
        "# Custom prior probabilities\n",
        "model_custom = GaussianNB(priors=[0.6, 0.4])\n",
        "model_custom.fit(X_train, y_train)\n",
        "y_pred_custom = model_custom.predict(X_test)\n",
        "print(\"Accuracy with custom priors:\", accuracy_score(y_test, y_pred_custom))"
      ],
      "metadata": {
        "id": "RuSBpBGMHAKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "40. Write a Python program to perform Recursive Feature Elimination (RFE) before training an SVM Classifier and compare accuracy.\n"
      ],
      "metadata": {
        "id": "n8ZMoAUCAJkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = datasets.load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(breast_cancer.data, breast_cancer.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM without feature elimination\n",
        "model = SVC(kernel='linear')\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy without RFE:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Perform Recursive Feature Elimination (RFE)\n",
        "selector = RFE(estimator=SVC(kernel='linear'), n_features_to_select=10)  # Select top 10 features\n",
        "X_train_rfe = selector.fit_transform(X_train, y_train)\n",
        "X_test_rfe = selector.transform(X_test)\n",
        "\n",
        "# Train SVM with selected features\n",
        "model_rfe = SVC(kernel='linear')\n",
        "model_rfe.fit(X_train_rfe, y_train)\n",
        "y_pred_rfe = model_rfe.predict(X_test_rfe)\n",
        "print(\"Accuracy with RFE:\", accuracy_score(y_test, y_pred_rfe))"
      ],
      "metadata": {
        "id": "9k66aVYuHCwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "41. Write a Python program to train an SVM Classifier and evaluate its performance using Precision, Recall, and F1-Score instead of accuracy.\n"
      ],
      "metadata": {
        "id": "maV0FWAkAOSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM Classifier\n",
        "model = SVC(kernel='linear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print results\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1)"
      ],
      "metadata": {
        "id": "3NaBJEBGHGBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "42. Write a Python program to train a Naïve Bayes Classifier and evaluate its performance using Log Loss (Cross-Entropy Loss).\n"
      ],
      "metadata": {
        "id": "fbzPwrgbAQEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = datasets.load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(breast_cancer.data, breast_cancer.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Gaussian Naïve Bayes\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_pred_proba = model.predict_proba(X_test)\n",
        "\n",
        "# Calculate Log Loss\n",
        "loss = log_loss(y_test, y_pred_proba)\n",
        "print(\"Log Loss:\", loss)"
      ],
      "metadata": {
        "id": "RZLNR5e0HIzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "43. Write a Python program to train an SVM Classifier and visualize the Confusion Matrix using seaborn.\n"
      ],
      "metadata": {
        "id": "M5AYseDnAR1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM Classifier\n",
        "model = SVC(kernel='linear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize Confusion Matrix\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ADOV5-CqHKz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "44. Write a Python program to train an SVM Regressor (SVR) and evaluate its performance using Mean Absolute Error (MAE) instead of MSE.\n"
      ],
      "metadata": {
        "id": "LN2o-kKEATqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Load the Boston Housing dataset\n",
        "boston = datasets.load_boston()\n",
        "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM Regressor\n",
        "model = SVR(kernel='linear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Mean Absolute Error\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"Mean Absolute Error:\", mae)"
      ],
      "metadata": {
        "id": "XRzp3tEtHNGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "45. Write a Python program to train a Naïve Bayes classifier and evaluate its performance using the ROC-AUC score.\n"
      ],
      "metadata": {
        "id": "iMtLjsLfAVl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = datasets.load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(breast_cancer.data, breast_cancer.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Gaussian Naïve Bayes\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate ROC-AUC Score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(\"ROC-AUC Score:\", roc_auc)"
      ],
      "metadata": {
        "id": "BNt8NutzHPRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "46. Write a Python program to train an SVM Classifier and visualize the Precision-Recall Curve."
      ],
      "metadata": {
        "id": "svNC1o42AXQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = datasets.load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(breast_cancer.data, breast_cancer.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM Classifier\n",
        "model = SVC(kernel='linear', probability=True)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Precision-Recall Curve\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
        "\n",
        "# Plot Precision-Recall Curve\n",
        "disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
        "disp.plot()\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IwZF9W7FHR9z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}